{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import focal_loss\n",
    "import torchvision.transforms.functional as transF\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''\n",
    "    CFG\n",
    "    Define hyperparameters used for preprocessing data and training model\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "        num_epochs: int \n",
    "            Number of epochs need for training.\n",
    "        batch_size: int\n",
    "            Number of samples used for training in 1 iteration.\n",
    "        num_workers: int\n",
    "            Number of processor used in multiprocessing training.\n",
    "        weight_decay: float (0 < weight_decay < 1)\n",
    "            Percentage of neuron dropped out.\n",
    "        mean: ndarray\n",
    "            Mean values of the whole dataset\n",
    "        std: ndarray\n",
    "            Standard deviation values of the whole dataset.\n",
    "        height: int\n",
    "            Height of input image used for the input layer of model.\n",
    "        width: int\n",
    "            Width of input image used for the input layer of model.\n",
    "        learning_rate: float\n",
    "            Step size at each iteration defines how much to change the model.\n",
    "        num_classes: int\n",
    "            Number of output classes.\n",
    "        target_names: list(str) \n",
    "            Names of output classes.\n",
    "    '''\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    weight_decay = 1e-5\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    height = 224\n",
    "    width = 224\n",
    "    learning_rate = 0.001\n",
    "    num_classes = 4\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    path = './archive/data'\n",
    "    stoi = {'cloudy' : 0, 'desert' : 1, 'green_area' : 2, 'water' : 3}\n",
    "    itos = {0 :'cloudy', 1 : 'desert', 2 : 'green_area', 3 : 'water'}\n",
    "    target_names = ['cloudy', 'desert', 'green_area', 'water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData:\n",
    "    def __init__(self, path = CFG.path):\n",
    "        self.path = path\n",
    "\n",
    "    def random_split_dataset(self):\n",
    "        '''\n",
    "        random_split_dataset(path = CFG.path)\n",
    "        Split the dataset into 3 subsets included: train, validation and test.\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, default=CFG.path\n",
    "            `path` define the path to dataset\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        tuple(list(str))\n",
    "            Tuple including 3 lists: train, validation, test. Each list contains paths to samples in dataset.\n",
    "            The default ratio for 3 subset: train=0.6, validation=0.2, test=0.2\n",
    "        '''\n",
    "        random.seed(42)\n",
    "        folders = os.listdir(self.path)\n",
    "        CFG.num_classes = len(folders)\n",
    "        data = []\n",
    "        CFG.le.fit(folders)\n",
    "        CFG.le_name_mapping = dict(zip(CFG.le.classes_, CFG.le.transform(CFG.le.classes_)))\n",
    "        for folder in folders:\n",
    "            for image_filename in os.listdir(os.path.join(self.path, folder)):\n",
    "                data.append((image_filename, folder))\n",
    "        print('num_classes', CFG.num_classes, 'length dataset', len(data))\n",
    "        CFG.train_size = int(0.8*len(data))\n",
    "        CFG.valid_size = int(0.1*len(data))\n",
    "        CFG.test_size = len(data) - CFG.train_size - CFG.valid_size\n",
    "        random.shuffle(data)\n",
    "        return data[ : CFG.train_size], data[CFG.train_size : CFG.train_size + CFG.valid_size], data[CFG.train_size + CFG.valid_size : ]\n",
    "\n",
    "    def get_transforms(self, transforms_type: str):\n",
    "        '''\n",
    "        get_transforms(transforms_type)\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        transforms_type: str\n",
    "            Refers to which dataset transformation applied on ('train', 'test', 'train')\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "            If mode is 'train': return a transform.Compose object including steps:\n",
    "                Resize, RandomHorizontalFlip, RandomRotation, ToTensor, Normalize\n",
    "            Else: return a transform.Compose object including steps:\n",
    "                Resize, ToTensor, Normalize\n",
    "        '''\n",
    "        if transforms_type == 'train':\n",
    "            return transforms.Compose([\n",
    "                    transforms.Resize((CFG.width, CFG.height)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(30),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=CFG.mean,std=CFG.std)\n",
    "                ])\n",
    "        else:\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((CFG.width, CFG.height)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=CFG.mean,std=CFG.std)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transforms_type = 'train'):\n",
    "        self.data = data\n",
    "        self.transforms = ProcessData().get_transforms(transforms_type)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        __getitem__(self, idx)\n",
    "        Get item by index\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx: int\n",
    "            Index of the sample in dataset.\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        A tuple of (image: tensor, label: numeric)\n",
    "        '''\n",
    "        image_filename, class_name = self.data[idx]\n",
    "        img_path = os.path.join(CFG.path, class_name, image_filename)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        return img, torch.tensor(CFG.le_name_mapping[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self):\n",
    "        self.process_data = ProcessData()\n",
    "\n",
    "    def get_loaders(self):\n",
    "        '''\n",
    "        get_loaders: \n",
    "            Split the dataset into 3 set: train, val and test. Each set will be converted into a CustomDataset object and transformed.\n",
    "            Divided each dataset into batches for data loading.\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        A tuple of (train_loader, val_loader, test_loader)\n",
    "        '''\n",
    "        train_data, val_data, test_data = self.process_data.random_split_dataset()\n",
    "        print(len(train_data), len(val_data), len(test_data))\n",
    "        train_dataset = CustomDataset(train_data, transforms_type = 'train')\n",
    "        val_dataset = CustomDataset(val_data, transforms_type = 'valid')\n",
    "        test_dataset = CustomDataset(test_data, transforms_type = 'test')\n",
    "        train_loader = DataLoader(dataset = train_dataset, batch_size = CFG.batch_size, shuffle = True)\n",
    "        val_loader = DataLoader(dataset = val_dataset, batch_size = CFG.batch_size, shuffle = False)\n",
    "        test_loader = DataLoader(dataset = test_dataset, batch_size = CFG.batch_size, shuffle = False)\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 4 length dataset 5631\n",
      "4504 563 564\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = DatasetLoader().get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(p=CFG.weight_decay)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 4)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 4)\n",
    "        self.conv3 = nn.Conv2d(12, 14, 4)\n",
    "        self.conv4 = nn.Conv2d(14, 16, 4)\n",
    "        self.conv5 = nn.Conv2d(16, 20, 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(20*4*4, 250)\n",
    "        self.fc2 = nn.Linear(250, 200)\n",
    "        self.fc3 = nn.Linear(200, 50)\n",
    "        self.fc4 = nn.Linear(50, 20)\n",
    "        self.fc5 = nn.Linear(20, CFG.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.reshape(-1, 20*4*4)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        self.model.train()\n",
    "        train_acc, correct_train, train_loss, target_count = 0, 0, 0, 0\n",
    "    \n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward and optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "                \n",
    "            # accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            target_count += labels.shape[0]\n",
    "            correct_train += (labels == predicted).sum().item()\n",
    "            train_acc = (100 * correct_train) / target_count\n",
    "        return train_acc, train_loss / target_count\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_acc, correct_val, val_loss, target_count = 0, 0, 0, 0\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            target_count += labels.shape[0]\n",
    "            correct_val += (labels == predicted).sum().item()\n",
    "            val_acc = (100 * correct_val) / target_count\n",
    "        return val_acc, val_loss / target_count \n",
    "    \n",
    "    def fit(self, model, the_last_loss = 100, patience = 10, trigger_times = 0, isStopped = False):\n",
    "        self.train_acc_history = []\n",
    "        self.train_loss_history = []\n",
    "        self.val_acc_history = []\n",
    "        self.val_loss_history = []\n",
    "\n",
    "        for epoch in range(0, CFG.num_epochs):\n",
    "            \n",
    "            train_acc, train_loss = self.train(train_loader)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            \n",
    "            val_acc,val_loss = self.validate(val_loader)\n",
    "            self.val_acc_history.append(val_acc)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "            \n",
    "            print(\"Epoch {}: train_acc {:.6f} \\t train_loss {:.6f} \\t val_acc {:.6f} \\t val_loss {:.6f}\".format(epoch+1, train_acc, train_loss, val_acc, val_loss))\n",
    "            print(\"Learning rate: {}\".format(self.optimizer.param_groups[0][\"lr\"]))\n",
    "            the_current_loss = val_loss\n",
    "            \n",
    "            if the_current_loss > the_last_loss:\n",
    "                trigger_times += 1\n",
    "                print('trigger times: ', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping! at epoch {0}'.format(epoch+1))\n",
    "                    isStopped = True\n",
    "                    break\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "                the_last_loss = the_current_loss\n",
    "                if not isStopped:\n",
    "                    torch.save(self.model.state_dict(), 'model.pt')\n",
    "                    print('Validation loss {:.6f}.  Saving model ...'.format(the_current_loss))\n",
    "\n",
    "    def plot_his_acc(self):\n",
    "        plt.figure(figsize=(25,5))\n",
    "        plt.plot(self.train_acc_history,'-o')\n",
    "        plt.plot(self.val_acc_history,'-o')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "        plt.legend(['Train_Acc','Val_Acc'])\n",
    "        plt.title('Train Acc and Val Acc')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_his_loss(self):\n",
    "        plt.figure(figsize=(25,5))\n",
    "        plt.plot(self.train_loss_history,'-o')\n",
    "        plt.plot(self.val_loss_history,'-o')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('losses')\n",
    "        plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "        plt.legend(['Train_Loss','Val_Loss'])\n",
    "        plt.title('Train Loss and Val Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/141 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\BAP_AI_Intern\\Ex for w5\\bap-intern\\problem3.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000012?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000012?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model)\n",
      "\u001b[1;32me:\\BAP_AI_Intern\\Ex for w5\\bap-intern\\problem3.ipynb Cell 9'\u001b[0m in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, the_last_loss, patience, trigger_times, isStopped)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=51'>52</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loss_history \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=53'>54</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, CFG\u001b[39m.\u001b[39mnum_epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=55'>56</a>\u001b[0m     train_acc, train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=56'>57</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_acc_history\u001b[39m.\u001b[39mappend(train_acc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=57'>58</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss_history\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32me:\\BAP_AI_Intern\\Ex for w5\\bap-intern\\problem3.ipynb Cell 9'\u001b[0m in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=14'>15</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(images)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=17'>18</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BAP_AI_Intern/Ex%20for%20w5/bap-intern/problem3.ipynb#ch0000009?line=19'>20</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\BAP_Intern_AI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\BAP_Intern_AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/loss.py?line=1161'>1162</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/loss.py?line=1162'>1163</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/loss.py?line=1163'>1164</a>\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/modules/loss.py?line=1164'>1165</a>\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\BAP_Intern_AI\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/functional.py?line=2993'>2994</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/functional.py?line=2994'>2995</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> <a href='file:///c%3A/Users/PC/anaconda3/envs/BAP_Intern_AI/lib/site-packages/torch/nn/functional.py?line=2995'>2996</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c652676742539b0910aca53298078d8c88630c998c955875479a7e5fae277ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('BAP_Intern_AI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
